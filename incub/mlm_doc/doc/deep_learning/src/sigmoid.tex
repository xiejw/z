\input src/format

%{{{1 Title
\sec{Sigmoid Function}
\mnote{2018-10-15}

%{{{1 Content
Sigmoid function has elegant gradient.

For a scalar $x$, typically a scalar logit from network, we have
%
$$
    \eqalign{
        \sigma(x)  & = { 1 \over 1 + \exp^{-x} } \cr
        \sigma'(x) & = \big( 1 - \sigma(x) \big) \sigma(x)
    }
$$
In neural network terminology, the output of the network is $z$, which is the
activation of the logit $x$.

However, if $x$ is very small, the $\sigma$ is not numerical stable. So, this
function should be calculated according to the condition $x>=0$ and $x<0$.

For binary-label classification problem, it is not good idea to calculate the
Sigmoid first and then the derivative. As the derivative of $\log$ functions
with a super small $z$, where $z=\sigma(x)$,  could be overflow. A more elegant
solution is generating the derivative from logits $x$ directly.

Say, the loss is defined as
%
$$
    \rm{Loss} = - y \log \sigma(x) - (1-y) \log \big(1-\sigma(x)\big).
$$
With this, the derivative of loss w.r.t.~$x$ could be:
%
$$
    \eqalign{
        {{\rm d} \rm{Loss} \over {\rm d} x}
            & = - y \big(1- \sigma(x)\big) + (1-y) \sigma(x) \cr
            & = \sigma(x) - y.
    }
$$

\vfill
\bye
