\input src/format

\sec{XGBoost Algorithrm}
\mnote{2024-12-29}

XGBoost \url{https://arxiv.org/abs/1603.02754} is a highly scalable gradient
boosting machine learning library. We will study its basic algorithm and then
the parallel optimization algorithm.

\subsec{Gradient Boosting Tree}

\mnote{Loss}
Let $\bar{x}$ be an $m$-feature input sample and $y$ be the coressponding real
value label.  Given an $N$-length input sequence $(\bar{x}_1, y_1), (\bar{x}_2,
y_2), \ldots, (\bar{x}_N,y_N)$, gradient boosting aims to optimize the function
approximation with an additive manner, i.e., after iteration $ K-1 $, train a
new $ f_{K }$ (usually a tree) such that for predictions
$$
{\hat{y}}_{i,(K)}
= {\hat{y}}_{i} = \phi_K(\bar{x}_i)
= \sum_{k=1}^K f_{k} (\bar{x}_i)
= \left[ \sum_{k=1}^{K-1} f_{k} (\bar{x}_i) \right] + f_K(\bar{x}_i)
$$
minimize the loss of
$$
\eqalignno{
L
&= \left[
    \sum_{i=1}^N l (y_i , \hat{y}_i)
  \right]
+ \Omega(\phi_K)
\cr
&= \left\{
    \sum_{i=1}^N l
      \left(
        y_i ,
        \left[
           \sum_{k=1}^{K-1} f_{k} (\bar{x}_i)
        \right] + f_K(\bar{x}_i)
      \right)
  \right\}
+ \Omega(\phi_K)
\cr
&= \left\{
    \sum_{i=1}^N l
      \left(
        y_i ,
        \hat{y}_{i,{(K-1)}} + f_K(\bar{x}_i)
      \right)
  \right\}
+ \Omega(\phi_K)
}
$$
where $l(\cdot)$ is a convex loss function and $\Omega$ is the regularization on
the number of leaves $T_{\{\cdot\}}$ and squares sum of all leaves weights
$w_{\{\cdot\},j}, \forall j \in\{1, 2, \ldots, T\}$ of all trees, i.e.,
$$
\eqalignno{
\Omega(\phi_K)
&
= \alpha \left( \sum_{k=1}^K T_k
  \right)
+ \gamma \left[
  \sum_{k=1}^K \left( \sum_{j=1}^{T_k} || w_{k,j} ||^2
               \right)
  \right]
\cr
%&
%= \left\{
%    \alpha
%      \left(
%        \sum_{k=1}^{K-1} T_k
%      \right)
%    + \gamma
%      \left[
%        \sum_{k=1}^{K-1}
%          \left( \sum_{j=1}^{T_k} || w_{k,j} ||^2
%          \right)
%      \right]
%  \right\}
%+ \left\{
%    \alpha
%      T_K
%    + \gamma
%        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
%        \right)
%  \right\}
%\cr
&
= \Omega(\phi_{K-1})
+ \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
}
$$

\mnote{Boosting}

The idea of boosting is to do approximation in the function space. This can be
done at each input sample $\bar{x}$ with the second order Taylor expansion
$$
\eqalignno{
  L_{(K)}
&= C_1 +
   \left\{
    \sum_{i=1}^N l
      \left(
        y_i ,
        \hat{y}_{i,{(K-1)}} + f_K(\bar{x}_i)
      \right)
  \right\}
+ \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
\cr
& \approx C_1 +
   \left\{
    \sum_{i=1}^N
      \left[
        l
        \left(
          y_i ,
          \hat{y}_{i,{(K-1)}}
        \right)
        + g_{i,(K-1)} f_K(\bar{x}_i)
        + {1\over2} h_{i,(K-1)} f_K^2(\bar{x}_i)
      \right]
  \right\}
\cr
&
\quad + \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
\cr
& = C_2 +
   \left\{
    \sum_{i=1}^N
      \left[
        g_{i,(K-1)} f_K(\bar{x}_i)
        + {1\over2} h_{i,(K-1)} f_K^2(\bar{x}_i)
      \right]
  \right\}
\cr
&
\quad + \left\{
    \alpha
      T_K
    + \gamma
        \left( \sum_{j=1}^{T_K} || w_{K,j} ||^2
        \right)
  \right\}
}
$$
where $g_i$ and $h_i$ are the first and second order of derivatives of the loss
funciton w.r.t. previous prediction $\hat{y}_{i,(K-1)}$. For a fixed tree
structure $f_K$, we can define $I_j$ to be the set of sample input indexes such
that they are mapped to the leave with index $j$, i.e.,
$$
I_j =
\left\{
  i | q(\bar{x}_i)=j
\right\},\quad \forall j = \{1,2,\ldots,T_K\}
$$
With this, we can rewrite the loss to be optimized as
$$
\eqalignno{
  L
& = C_2 + \alpha T
+ \sum_{j=1}^{T}
 \left\{
   \left( \sum_{i\in I_j} g_i \right) w_j
   +
   {1\over2}
   \left[
     \left(
     \sum_{i\in I_j} h_i
     \right)
     + \gamma
    \right] w_j^2
  \right\}
}
$$
where the iteration $K$ is removed from the equation toward simplicity. Note
that both $g_i$ and $h_i$ are functions over $\bar{x}_{i}$ determined by the
boosting tree essembled in the $K-1$ iteration, so there are constants during
optimization in iteration $K$.

This loss $L$ has minimal value, for a fixed tree structure, as
$$
\eqalignno{
  L^*
& = C_2 + \alpha T
-
 {1\over2}
 \sum_{j=1}^{T}
 {
   \left( \sum_{i\in I_j} g_i \right)^2
   \over
   \left[
     \left(
     \sum_{i\in I_j} h_i
     \right)
     + \gamma
    \right]
  }
}
$$
And in order to greedily split the tree, we can focus one leave $j$ at a time,
followed by going through
all feature dimensions (total $m$ of them), and then, for each dimension $d$,
going through (sorted) sample input values ($\{x_{i,d} | \forall i\}$) to find a split
$I_{j,(d,L)}$ and $I_{j,(d,R)}$, such that the loss reduction is
$$
\eqalignno{
  L^*_{j, reduction}
& = - \alpha
+
  {1 \over 2}
  \left[
    {
      \left( \sum_{i\in I_{j,(d,L)}} g_i \right)^2
    \over
      \left(
      \sum_{i\in I_{j,(d,L)}} h_i
      \right)
      + \gamma
    }
    +
    {
      \left( \sum_{i\in I_{j,(d,R)}} g_i \right)^2
    \over
      \left(
      \sum_{i\in I_{j,(d,R)}} h_i
      \right)
      + \gamma
    }
    -
    {
      \left( \sum_{i\in I_{j}} g_i \right)^2
    \over
      \left(
      \sum_{i\in I_{j}} h_i
      \right)
      + \gamma
    }
  \right]
}
$$

\subsec{Distributed Algorithm}

Let's focus on single feature first, $m=1$. In order to find the best split, we
need
- focus on each node $2^D$
- sort all the nodes in that node $O(n_i\log n_i)$.
- go throught all nodes in order one by one and identify the best split
  resulting to the best loss reduction $O(n_i)$.

This is called exact splitting.

Node level parallel is not very scalable and often has very poor load balancing.
The second loop is terribly bad apparantly. Not only sorting in one machine is
very challenging for super large dataset, but also all machines need to read all
data, which are super slow, and inefficient.

A quite smart way is to use approximation splitting. Instead of splitting at the
features in the datasets, we could split at the some pencentiles approximation
points with estimated $g_i$ and $h_i$. To be specific, with a hyper parameters
of the bin size $B$, all nodes are put into the bins and each bin record the
average values of $g_i$ and $h_i$. This is the histgram which can be built in a
distributed manner (see ref1 and ref2). When a center driver merges the
histgram, it can estimate the percentile positions and decide the approximation
split.

Then the algorithm is the following
- For depth level $l$, $l<=4$. $4$
- Each machine only reads only the non-overlapping data partitions belong it and then
  map the input to the node at depth $l$, it builds the histgram for each
  feature dimension, so total of $m$ histgrams.
- Distribute histgrams to driver and the driver will merge them and then decide
  the split. Driver broadcasts this info to all machines. So $l+1$ level is built.
- Repeat for next depth level.

2024-12-22 Streaming Parallel Decision Tree Algorithm \url{https://www.jmlr.org/papers/v11/ben-haim10a.html}
Parallel Boosted Regression Trees for Web Search Ranking \url{https://www.cs.cornell.edu/~kilian/papers/fr819-tyreeA.pdf}
\url{https://ieeexplore.ieee.org/document/8890990}

\vfill
\bye
